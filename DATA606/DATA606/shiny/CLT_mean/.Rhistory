pis[2]*dnorm(x, mean = mu[2], sd = sigma[2])))
z[which(z==0)] = 2
## update pi
a = sum(z == 1)
b = sum(z == 2)
n = c(a,b)
pis = rdirichlet(1, a.0+n)
## update mu
mu.star = rnorm(k,mean=mu,sd=.01)
for (j in 1:k){
test.mu = mu
test.mu[j] = mu.star[j]
accept = prod(mapply(
function(a,b)
dnorm(a, mean = test.mu[b], sd = sigma[b]) /
dnorm(a, mean = mu[b], sd = sigma[b]), x,z)) *
prod(dunif(test.mu,0,5)/dunif(mu,0,5))
r = min(1,accept)
u = runif(1, min=0, max=1)
if (u < r){
mu = test.mu
}
}
## update sigma using inverse gamma
alpha = (alpha.0 + sapply(1:2, function(y) sum(z == y)))/2
beta = beta.0 + sapply(1:2, function(y) (sum((z==y) * (x-mu[y])^2))/2)
sigma[1] = sqrt(rigamma(1, alpha[1], beta[1]))
sigma[2] = sqrt(rigamma(1, alpha[2], beta[2]))
print(i)
if (i > burn){
store.z[i-burn,] = c(table(z)[1], table(z)[2])
store.pis[i-burn,] = pis
means[i-burn,] = mu
store.sigma[i-burn,] = sigma
dll.2[i-burn] = sum(log(
mapply(function(a,b) dnorm(a, mean = mu[b], sd = sigma[b]), x, z)))
}
}
plot(1:(nIter-burn), dll.2, main = "Log Likelihood Trace (2)", xlab = "Iteration", ylab = "Log Likelihood")
hist(means[,1], main = expression(mu[1]), xlab = "Sample means")
hist(means[,2], main = expression(mu[2]), xlab = "Sample means")
plot(1:(nIter-burn), means[,1], ylim = c(0,5))
points(1:(nIter-burn), means[,2], col = "cadetblue")
### part c
x = HW6_mixture$V1
nIter = 10000
burn = 1000
k=2
## store variables
## dll is vector of data log likelihoods at each iteration
store.z = matrix(0, nrow = nIter-burn, ncol = k)
store.pis = matrix(0, nrow = nIter-burn, ncol = k)
means = matrix(0, nrow = nIter-burn, ncol = k)
store.sigma = matrix(0, nrow = nIter-burn, ncol = k)
dll.1 = NA
dll.2 = NA
dll.3 = NA
## set constants
a.0 = c(.5,.5)
alpha.0 = 1 ; beta.0 = 1
## set initial values (draws from prior)
z = sample(c(1:2), length(x), replace = TRUE)
pis = rdirichlet(1, a.0)
#mu = runif(2,0,5)
mu = c(2.5,2.5)
sigma = sqrt(rigamma(2,alpha.0,beta.0))
for (i in 1:nIter){
## update z
z = rbinom(length(x), 1,
pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])/
(pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])+
pis[2]*dnorm(x, mean = mu[2], sd = sigma[2])))
z[which(z==0)] = 2
## update pi
a = sum(z == 1)
b = sum(z == 2)
n = c(a,b)
pis = rdirichlet(1, a.0+n)
## update mu
mu.star = rnorm(k,mean=mu,sd=.01)
for (j in 1:k){
test.mu = mu
test.mu[j] = mu.star[j]
accept = prod(mapply(
function(a,b)
dnorm(a, mean = test.mu[b], sd = sigma[b]) /
dnorm(a, mean = mu[b], sd = sigma[b]), x,z)) *
prod(dunif(test.mu,0,5)/dunif(mu,0,5))
r = min(1,accept)
u = runif(1, min=0, max=1)
if (u < r){
mu = test.mu
}
}
## update sigma using inverse gamma
alpha = (alpha.0 + sapply(1:2, function(y) sum(z == y)))/2
beta = beta.0 + sapply(1:2, function(y) (sum((z==y) * (x-mu[y])^2))/2)
sigma[1] = sqrt(rigamma(1, alpha[1], beta[1]))
sigma[2] = sqrt(rigamma(1, alpha[2], beta[2]))
#sigma = sqrt(1/rgamma(2, alpha, beta))
print(i)
if (i > burn){
store.z[i-burn,] = c(table(z)[1], table(z)[2])
store.pis[i-burn,] = pis
means[i-burn,] = mu
store.sigma[i-burn,] = sigma
dll.3[i-burn] = sum(log(
mapply(function(a,b) dnorm(a, mean = mu[b], sd = sigma[b]), x, z)))
}
}
plot(1:(nIter-burn), dll.3, main = "Log Likelihood Trace (3)", xlab = "Iteration", ylab = "Log Likelihood")
mu
pi
store.pis[1]
store.pis[1,]
pis
x = HW6_mixture$V1
nIter = 10000
burn = 1000
k=2
## store variables
## dll is vector of data log likelihoods at each iteration
store.z = matrix(0, nrow = nIter-burn, ncol = k)
store.pis = matrix(0, nrow = nIter-burn, ncol = k)
means = matrix(0, nrow = nIter-burn, ncol = k)
store.sigma = matrix(0, nrow = nIter-burn, ncol = k)
dll.1 = NA
dll.2 = NA
dll.3 = NA
## set constants
a.0 = c(.5,.5)
alpha.0 = 1 ; beta.0 = 1
## set initial values (draws from prior)
pis = rdirichlet(1, a.0)
mu = runif(2,0,5)
#mu = c(2.5,2.5)
sigma = sqrt(rigamma(2,alpha.0,beta.0))
params = c(pis, mu, sigma)
for (i in 1:nIter){
## update z
z = rbinom(length(x), 1,
pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])/
(pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])+
pis[2]*dnorm(x, mean = mu[2], sd = sigma[2])))
z[which(z==0)] = 2
## update pi
a = sum(z == 1)
b = sum(z == 2)
n = c(a,b)
pis = rdirichlet(1, a.0+n)
## update mu
mu.star = rnorm(k,mean=mu,sd=.01)
for (j in 1:k){
test.mu = mu
test.mu[j] = mu.star[j]
accept = prod(mapply(
function(a,b)
dnorm(a, mean = test.mu[b], sd = sigma[b]) /
dnorm(a, mean = mu[b], sd = sigma[b]), x,z)) *
prod(dunif(test.mu,0,5)/dunif(mu,0,5))
r = min(1,accept)
u = runif(1, min=0, max=1)
if (u < r){
mu = test.mu
}
}
## update sigma using inverse gamma
alpha = (alpha.0 + sapply(1:2, function(y) sum(z == y)))/2
beta = beta.0 + sapply(1:2, function(y) (sum((z==y) * (x-mu[y])^2))/2)
sigma[1] = sqrt(rigamma(1, alpha[1], beta[1]))
sigma[2] = sqrt(rigamma(1, alpha[2], beta[2]))
#sigma = sqrt(1/rgamma(2, alpha, beta))
print(i)
if (i > burn){
store.z[i-burn,] = c(table(z)[1], table(z)[2])
store.pis[i-burn,] = pis
means[i-burn,] = mu
store.sigma[i-burn,] = sigma
dll.1[i-burn] = sum(log(
mapply(function(a,b) dnorm(a, mean = mu[b], sd = sigma[b]), x, z)))
}
}
plot(1:(nIter-burn), dll.1, main = "Log Likelihood Trace (1)", xlab = "Iteration", ylab = "Log Likelihood")
params
### part c
x = HW6_mixture$V1
nIter = 10000
burn = 1000
k=2
## store variables
## dll is vector of data log likelihoods at each iteration
store.z = matrix(0, nrow = nIter-burn, ncol = k)
store.pis = matrix(0, nrow = nIter-burn, ncol = k)
means = matrix(0, nrow = nIter-burn, ncol = k)
store.sigma = matrix(0, nrow = nIter-burn, ncol = k)
dll.1 = NA
dll.2 = NA
dll.3 = NA
## set constants
a.0 = c(.5,.5)
alpha.0 = 1 ; beta.0 = 1
## set initial values (draws from prior)
pis = rdirichlet(1, a.0)
mu = runif(2,0,5)
sigma = sqrt(rigamma(2,alpha.0,beta.0))
params = c(pis, mu, sigma)
for (i in 1:nIter){
## update z
z = rbinom(length(x), 1,
pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])/
(pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])+
pis[2]*dnorm(x, mean = mu[2], sd = sigma[2])))
z[which(z==0)] = 2
## update pi
a = sum(z == 1)
b = sum(z == 2)
n = c(a,b)
pis = rdirichlet(1, a.0+n)
## update mu
mu.star = rnorm(k,mean=mu,sd=.01)
for (j in 1:k){
test.mu = mu
test.mu[j] = mu.star[j]
accept = prod(mapply(
function(a,b)
dnorm(a, mean = test.mu[b], sd = sigma[b]) /
dnorm(a, mean = mu[b], sd = sigma[b]), x,z)) *
prod(dunif(test.mu,0,5)/dunif(mu,0,5))
r = min(1,accept)
u = runif(1, min=0, max=1)
if (u < r){
mu = test.mu
}
}
## update sigma using inverse gamma
alpha = (alpha.0 + sapply(1:2, function(y) sum(z == y)))/2
beta = beta.0 + sapply(1:2, function(y) (sum((z==y) * (x-mu[y])^2))/2)
sigma[1] = sqrt(rigamma(1, alpha[1], beta[1]))
sigma[2] = sqrt(rigamma(1, alpha[2], beta[2]))
#sigma = sqrt(1/rgamma(2, alpha, beta))
print(i)
if (i > burn){
store.z[i-burn,] = c(table(z)[1], table(z)[2])
store.pis[i-burn,] = pis
means[i-burn,] = mu
store.sigma[i-burn,] = sigma
dll.1[i-burn] = sum(log(
mapply(function(a,b) dnorm(a, mean = mu[b], sd = sigma[b]), x, z)))
}
}
plot(1:(nIter-burn), dll.1, main = "Log Likelihood Trace (1)", xlab = "Iteration", ylab = "Log Likelihood")
dll.2 = dll.1
plot(1:(nIter-burn), dll.2, main = "Log Likelihood Trace (2)", xlab = "Iteration", ylab = "Log Likelihood")
params
pi
pis
sigma
mu
hist(means[,1], main = expression(mu[1]), xlab = "Sample means")
hist(means[,2], main = expression(mu[2]), xlab = "Sample means")
### part c
x = HW6_mixture$V1
nIter = 10000
burn = 1000
k=2
## store variables
## dll is vector of data log likelihoods at each iteration
store.z = matrix(0, nrow = nIter-burn, ncol = k)
store.pis = matrix(0, nrow = nIter-burn, ncol = k)
means = matrix(0, nrow = nIter-burn, ncol = k)
store.sigma = matrix(0, nrow = nIter-burn, ncol = k)
dll.1 = NA
dll.2 = NA
dll.3 = NA
## set constants
a.0 = c(.5,.5)
alpha.0 = 1 ; beta.0 = 1
## set initial values (draws from prior)
pis = rdirichlet(1, a.0)
mu = runif(2,0,5)
sigma = sqrt(rigamma(2,alpha.0,beta.0))
params = c(pis, mu, sigma)
for (i in 1:nIter){
## update z
z = rbinom(length(x), 1,
pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])/
(pis[1]*dnorm(x, mean = mu[1], sd = sigma[1])+
pis[2]*dnorm(x, mean = mu[2], sd = sigma[2])))
z[which(z==0)] = 2
## update pi
a = sum(z == 1)
b = sum(z == 2)
n = c(a,b)
pis = rdirichlet(1, a.0+n)
## update mu
mu.star = rnorm(k,mean=mu,sd=.01)
for (j in 1:k){
test.mu = mu
test.mu[j] = mu.star[j]
accept = prod(mapply(
function(a,b)
dnorm(a, mean = test.mu[b], sd = sigma[b]) /
dnorm(a, mean = mu[b], sd = sigma[b]), x,z)) *
prod(dunif(test.mu,0,5)/dunif(mu,0,5))
r = min(1,accept)
u = runif(1, min=0, max=1)
if (u < r){
mu = test.mu
}
}
## update sigma using inverse gamma
alpha = (alpha.0 + sapply(1:2, function(y) sum(z == y)))/2
beta = beta.0 + sapply(1:2, function(y) (sum((z==y) * (x-mu[y])^2))/2)
sigma[1] = sqrt(rigamma(1, alpha[1], beta[1]))
sigma[2] = sqrt(rigamma(1, alpha[2], beta[2]))
#sigma = sqrt(1/rgamma(2, alpha, beta))
print(i)
if (i > burn){
store.z[i-burn,] = c(table(z)[1], table(z)[2])
store.pis[i-burn,] = pis
means[i-burn,] = mu
store.sigma[i-burn,] = sigma
dll.3[i-burn] = sum(log(
mapply(function(a,b) dnorm(a, mean = mu[b], sd = sigma[b]), x, z)))
}
}
plot(1:(nIter-burn), dll.3, main = "Log Likelihood Trace (3)", xlab = "Iteration", ylab = "Log Likelihood")
params
hist(means[,1], main = expression(mu[1]), xlab = "Sample means")
hist(means[,2], main = expression(mu[2]), xlab = "Sample means")
mu
hist(means[,1], main = expression(mu[1]), xlab = "Sample means")
mean(means[,1])
mean(means[,2])
sigma
sigma^2
.59^2
?rdirichlet
library(shiny)
library(openintro)
library(ggplot2)
library(gridExtra)
n=10
k=50
mu=0
sd=1
n <- 150 # number of data points
p <- 2   # dimension
sigma <- 1  # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
npos
nneg
dim(xpos)
dim(xneg)
xpos[1,]
xneg[1,]
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
y
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
legend("topleft",c("Positive","Negative"),col=seq(2),pch=1,text.col=seq(2))
?ifelse
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ntrain
tindex <- sample(n,ntrain) # indices of training samples
tindex
xtrain <- x[tindex,]
dim(xtrain)
xtest <- x[-tindex,]
dim(xtest)
ytrain
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
istrain
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c(’Positive Train’,’Positive Test’,’Negative Train’,’Negative Test’),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
legend("topleft",c("Positive Train","Positive Test","Negative Train","Negative Test"),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
library(kernlab)
install.packages(kernlab)
install.packages("kernlab")
library(kernlab)
?ksvm
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel=’vanilladot’,C=100,scaled=c())
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel="vanilladot",C=100,scaled=c())
svp
# Attributes that you can access
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
plot(svp,data=xtrain)
test100 <- read.delim("~/Dropbox/ML Project (1)/test100.txt")
View(test100)
library("e1071", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(e1071)
library(MASS)
data(cats)
model <- svm(Sex~., data=cats)
print(model)
summary(model)
model
help(predict.s.vm)
help(predict.svm)
?ksvm
test.data = as.matrix(test100)
y = test100$label
type(y)
typeof(y)
y = as.factor(test100$label)
y
model <- kvsm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
names(test100)
test.data = as.matrix(test100[6:26,])
dim(test.data)
test.data = as.matrix(test100[,6:26])
dim(test.data)
dim(test100)
y = as.factor(test100$label)
length(y)
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
names(test100)
names(test.data)
names(test100[,6:26])
table(is.na(test100))
table(is.na(test100[,6:26]))
table(is.na(test100[,6]))
test.data = as.matrix(test100[1:94,6:26])
dim(test.data)
y = as.factor(test100$label)
y = as.factor(test100$label[1:94])
length(y)
y
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
table(is.na(test100[,6]))
table(is.na(test100[1:94,6]))
table(is.na(test100[1:94,6]))
table(is.na(test100[1:94,7]))
table(is.na(test100[1:94,8]))
table(is.na(test100[1:94,9]))
table(is.na(test100[1:94,1-]))
table(is.na(test100[1:94,10]))
table(is.na(test100[1:94,11]))
table(is.na(test100[1:94,12]))
table(is.na(test100[1:94,13]))
table(is.na(test100[1:94,14]))
table(is.na(test100[1:94,15]))
table(is.na(test100[1:94,16]))
table(is.na(test100[1:94,17]))
table(is.na(test100[1:94,18]))
table(is.na(test100[1:94,19]))
table(is.na(test100[1:94,20]))
table(is.na(test100[1:94,21]))
table(is.na(test100[1:94,22]))
table(is.na(test100[1:94,23]))
table(is.na(test100[1:94,24]))
table(is.na(test100[1:94,25]))
table(is.na(test100[1:94,26]))
for (i in 1:nrow(test100)){
for (j in 1:ncol(test100)){
if (test100[i,j] == "?") test100[i,j] = NA
}
}
for (i in 1:nrow(test100)){
for (j in 1:ncol(test100)){
if (test100[i,j] = "?") test100[i,j] = NA
}
}
nrow(test100)
ncol(test100)
alchemy_category[1,]
test100$alchemy_category[1,]
test100$alchemy_category[,1]
test100$alchemy_category[1]
test100$alchemy_category[2]
names(test100)
test100 = test100[1:94,]
View(test100)
test100 = as.matrix(test100[,6:26])
test100 = test100[1:94,]
test.data = as.matrix(test100[,6:26])
test.data = as.matrix(test.data[,6:26])
test100 <- read.delim("~/Dropbox/ML Project (1)/test100.txt")
View(test100)
test100 = test100[1:94,]
test.data = as.matrix(test100[,6:26])
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
table(is.na(test100[1:94,26]))
names(test100)
test.data = cbind(test100[,6:17],test100[,18:26])
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="vanilladot")
model <- ksvm(x=test.data, y=y, type="C-svc", kernel="rbfdot")
model
length
model <- ksvm(x=test.data, y=y, type="C-svc")
setwd("~/Dropbox/Shiny")
library(shiny)
runApp("CLT")
