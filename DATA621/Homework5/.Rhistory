get an idea what distribution the features follow and see the data for users who
were in a crash and those who were not.
```{r plot-hist-data, echo=FALSE, warning=FALSE}
# get histogram of the numerical/integer data first
ggplot(gather(insurance_train[, c(2:7, 15, 18, 22, 24:25)]), aes(x=value)) +
geom_histogram(bins = 10) +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Numerical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6))
# get plot of the categorical variables
ggplot(gather(insurance_train[, c(9, 11:14, 16, 19:20, 23, 26)]), aes(x=value)) +
geom_bar() +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Categorical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6),
axis.text.x = element_text(angle = 45, hjust = 1))
```
## Build Models
- Let's build our multiple linear regression and logistic binary regression to
predict the amount cost for a car crash and if a person was in one or not respectively.
- The first linear regression model will be using weighted linear regression that
is applying weighted values.
```{r model1, echo=FALSE}
# get rid of the index it is no use to us for this analysis
insurance_train <- insurance_train %>% select(-c(INDEX))
# use a model that includes all variables and do a prediction
# the squared inverse of these predictions will be used as weights for the features
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train)
pred <- predict(lm1_insurance, newdata=insurance_train) # make predictions
# re-fit model but using weights
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train, weights = abs(pred))
summary(lm1_insurance)
par(mfrow=c(2,2))
plot(lm1_insurance)
```
- Model 2 is the usage of backward elimination to see which variables to eliminate
with high p-values
```{r model2, echo=FALSE}
lm2_insurance <- step(lm1_insurance, direction = "backward", trace = FALSE)
summary(lm2_insurance)
par(mfrow=c(2,2))
plot(lm2_insurance)
```
### Making Binary Logistic Regression Models
- First Binary Logistic Regression model will be using the step() function to select
features using backward selection and then doing a logistic
regression.
```{r logistic1, echo=FALSE}
# make a logistic regression model then use step() to see which variables
# are most desirable to be used
insurance_train_subset <- insurance_train %>% select(-c(TARGET_AMT))
lm1_log_insurance <- glm(TARGET_FLAG ~.,
data = insurance_train_subset, family = "binomial")
lm1_log_insurance <- stepAIC(lm1_log_insurance, direction = "backward", trace = FALSE)
summary(lm1_log_insurance)
```
- Using Backward elimination for logistic regression gives reduced variables and
converges quite fast. Let's try another model and
the coefficents and y-intercept are below. This is using the LASSO regression
techinque which prevents overfitting and penalizes coefficents in the model.
```{r logistic2, echo=FALSE}
lm2_log_insurance <- cv.glmnet(data.matrix(insurance_train[, c(3:25)]),
y = insurance_train$TARGET_FLAG, family = "binomial",
type.measure = "class")
# coefficents
plot(lm2_log_insurance)
fit <- glmnet(data.matrix(insurance_train[, c(3:25)]),
y = insurance_train$TARGET_FLAG, family = "binomial",
alpha = 1, lambda = lm2_log_insurance$lambda.1se)
# Fitted coefficents
fit$a0
fit$beta[,1][fit$beta[,1] != 0]
# computing the AIC
k <- fit$df # number of features, degrees of freedom
loglikhood <- fit$nulldev - deviance(fit) # log-likihood
AICc <- -2*loglikhood + 2*k
AICc
```
- This model tells us that around 18 variables will make this model have minimum
missclassification error rate and provide a good value for lambda the regularization
parameter. This model looks good some variables were removed and one that I think that
should have been removed was the AGE variable. I strongly believe that age plays
a factor in car crashes. Usually 20-30 is a high chance as well as seniors (60+)
Also I believe the RED_CAR is not relevant; it should not matter if the car was a
red car or not.
- Model 3 I will manually select variables I think would best contribute to predicting
if someone was in a car crash or not. They are:
- AGE
- CAR_USE
- CLM_FREQ
- KIDSDRIV
- MVR_PTS
- OLDCLAIM
- REVOKED
- SEX
- TIF
- TRAVTIME
- Other factors like type of job, education, type of car, value of car in my
opinion don't contribute to car crashes or lack thereof. Any educated person can
be in a car crash and crashes often happen regardless of the value of car.
```{r logistic3, echo=FALSE}
lm3_model_insurance <- glm(TARGET_FLAG ~ AGE + CAR_USE + CLM_FREQ +
KIDSDRIV + MVR_PTS + OLDCLAIM + REVOKED + SEX + TIF +
TRAVTIME, data = insurance_train, family = "binomial")
summary(lm3_model_insurance)
```
## Selecting Models
- For the logistic regression I will be selecting Model 2 that is using the LASSO
regularization technique using glmnet/cv.glmnet functions. The AIC is the lowest
out of the three and one thing that I have noticed is that the variables that you
think would make the cut is not always the best model. Some variables were left out
as mentioned previously but from experience using LASSO with regularization gives good
classification and accuracy.
- For the multiple linear regression model, a tough decision but I will go with
model 2 that is the weighted least squares with backward elimination. This results
in less features a slighly but barely higher $R_{adj}^2$. The $R_{adj}^2$ isn't
the best and feel that more advanced non-linear techniques need to be learned and can
maybe provide a better estimation.
- Let's evaluate the multiple linear regression model # 2 on the training dataset
a. Mean Squared Error (MSE)
```{r mse, echo=FALSE}
sm <- summary(lm2_insurance)
mse <- mean(sm$residuals^2)
mse
```
b. $R^2$ (or $R_{adj}^2$)
```{r r-squared, echo=FALSE}
sm$r.squared
```
c. F-Statistic
```{r f-stat, echo=FALSE}
sm$fstatistic[1]
```
d. Residual Plots
```{r res-plots, echo=FALSE}
par(mfrow=c(2,2))
plot(lm2_insurance)
```
- Evaluating the binary logistic regression model using the training set.
```{r evaluation, echo=FALSE}
# predict on the training data to compute stats
predict_train <- predict(fit, type = "class", newx = data.matrix(insurance_train[,3:25]),
s = "lambda.1se")
table(predict_train)
table(insurance_train$TARGET_FLAG)
conf_matrix <- confusionMatrix(factor(predict_train), insurance_train$TARGET_FLAG,
positive = "1")
conf_matrix
conf_matrix_table <- conf_matrix$table
```
# predict on the training data to compute stats
predict_train <- predict(fit, type = "class", newx = data.matrix(insurance_train[,3:25]),
s = "lambda.1se")
table(predict_train)
table(insurance_train$TARGET_FLAG)
conf_matrix <- confusionMatrix(factor(predict_train), insurance_train$TARGET_FLAG,
positive = "1")
conf_matrix
conf_matrix_table <- conf_matrix$table
if (!require(MASS)) install.packages("MASS", dependencies = TRUE)
if (!require(ggplot2)) install.packages("ggplot2", dependencies = TRUE)
if (!require(dplyr)) install.packages("dplyr", dependencies = TRUE)
if (!require(caret)) install.packages("caret", dependencies = TRUE)
if (!require(glmnet)) install.packages("glmnet", dependencies = TRUE)
if (!require(tidyr)) install.packages("tidyr", dependencies = TRUE)
# load libraries
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)
library(MASS)
# load dataset (training data)
insurance_train <- read.csv("insurance_training_data.csv")
f1 <- (2 * precision(conf_matrix_table) * sensitivity(conf_matrix_table)) /
(precision(conf_matrix_table) + specificity(conf_matrix_table))
if (!require(MASS)) install.packages("MASS", dependencies = TRUE)
if (!require(ggplot2)) install.packages("ggplot2", dependencies = TRUE)
if (!require(dplyr)) install.packages("dplyr", dependencies = TRUE)
if (!require(caret)) install.packages("caret", dependencies = TRUE)
if (!require(glmnet)) install.packages("glmnet", dependencies = TRUE)
if (!require(tidyr)) install.packages("tidyr", dependencies = TRUE)
# load libraries
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)
library(MASS)
# load dataset (training data)
insurance_train <- read.csv("insurance_training_data.csv")
str(insurance_train)
dim(insurance_train)
summary(insurance_train)
# remove the '$' and ',' in the INCOME, HOME_VAL and BLUEBOOK and OLDCLAIM for
# analysis
insurance_train$INCOME <- as.numeric(gsub('\\$|,', '', insurance_train$INCOME))
insurance_train$HOME_VAL <- as.numeric(gsub('\\$|,', '', insurance_train$HOME_VAL))
insurance_train$BLUEBOOK <- as.numeric(gsub('\\$|,', '', insurance_train$BLUEBOOK))
insurance_train$OLDCLAIM <- as.numeric(gsub('\\$|,', '', insurance_train$OLDCLAIM))
# mean of each column dealing with currency
cols_na <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM", "AGE", "YOJ", "CAR_AGE")
means <- apply(insurance_train[, cols_na],
2, mean, na.rm = TRUE)
insurance_train$INCOME <- insurance_train$INCOME %>% replace_na(means[1])
insurance_train$HOME_VAL <- insurance_train$HOME_VAL %>% replace_na(means[2])
insurance_train$BLUEBOOK <- insurance_train$BLUEBOOK %>% replace_na(means[3])
insurance_train$OLDCLAIM <- insurance_train$OLDCLAIM %>% replace_na(means[4])
insurance_train$AGE <- insurance_train$AGE %>% replace_na(means[5])
insurance_train$YOJ <- insurance_train$YOJ %>% replace_na(means[6])
insurance_train$CAR_AGE <- insurance_train$CAR_AGE %>% replace_na(means[7])
insurance_train$CAR_AGE[insurance_train$CAR_AGE == -3] <- 0
# categorical variables to have z_ in the value replaced
categorical_vars <-
c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")
# replace the z_ with the empty string using apply and gsub
insurance_train[, categorical_vars] <-
apply(insurance_train[, categorical_vars], 2,
function(x) as.factor(gsub("z_", "", x)))
insurance_train[, categorical_vars] <- lapply(insurance_train[, categorical_vars],
factor)
# replace "" a factor label in JOB and replace with other to mean other jobs
levels(insurance_train$JOB)[levels(insurance_train$JOB) == ""] <- "Other"
# get histogram of the numerical/integer data first
ggplot(gather(insurance_train[, c(2:7, 15, 18, 22, 24:25)]), aes(x=value)) +
geom_histogram(bins = 10) +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Numerical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6))
# get plot of the categorical variables
ggplot(gather(insurance_train[, c(9, 11:14, 16, 19:20, 23, 26)]), aes(x=value)) +
geom_bar() +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Categorical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6),
axis.text.x = element_text(angle = 45, hjust = 1))
lm2_insurance <- step(lm1_insurance, direction = "backward", trace = FALSE)
# get rid of the index it is no use to us for this analysis
insurance_train <- insurance_train %>% select(-c(INDEX))
# use a model that includes all variables and do a prediction
# the squared inverse of these predictions will be used as weights for the features
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train)
pred <- predict(lm1_insurance, newdata=insurance_train) # make predictions
# re-fit model but using weights
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train, weights = abs(pred))
summary(lm1_insurance)
par(mfrow=c(2,2))
plot(lm1_insurance)
lm2_insurance <- step(lm1_insurance, direction = "backward", trace = FALSE)
summary(lm2_insurance)
par(mfrow=c(2,2))
plot(lm2_insurance)
sm <- summary(lm2_insurance)
mse <- mean(sm$residuals^2)
mse
sm$r.squared
sm$fstatistic[1]
par(mfrow=c(2,2))
plot(lm2_insurance)
# predict on the training data to compute stats
predict_train <- predict(fit, type = "class", newx = data.matrix(insurance_train[,3:25]),
s = "lambda.1se")
if (!require(MASS)) install.packages("MASS", dependencies = TRUE)
if (!require(ggplot2)) install.packages("ggplot2", dependencies = TRUE)
if (!require(dplyr)) install.packages("dplyr", dependencies = TRUE)
if (!require(caret)) install.packages("caret", dependencies = TRUE)
if (!require(glmnet)) install.packages("glmnet", dependencies = TRUE)
if (!require(tidyr)) install.packages("tidyr", dependencies = TRUE)
# load libraries
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)
library(MASS)
# load dataset (training data)
insurance_train <- read.csv("insurance_training_data.csv")
str(insurance_train)
dim(insurance_train)
summary(insurance_train)
# remove the '$' and ',' in the INCOME, HOME_VAL and BLUEBOOK and OLDCLAIM for
# analysis
insurance_train$INCOME <- as.numeric(gsub('\\$|,', '', insurance_train$INCOME))
insurance_train$HOME_VAL <- as.numeric(gsub('\\$|,', '', insurance_train$HOME_VAL))
insurance_train$BLUEBOOK <- as.numeric(gsub('\\$|,', '', insurance_train$BLUEBOOK))
insurance_train$OLDCLAIM <- as.numeric(gsub('\\$|,', '', insurance_train$OLDCLAIM))
# mean of each column dealing with currency
cols_na <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM", "AGE", "YOJ", "CAR_AGE")
means <- apply(insurance_train[, cols_na],
2, mean, na.rm = TRUE)
insurance_train$INCOME <- insurance_train$INCOME %>% replace_na(means[1])
insurance_train$HOME_VAL <- insurance_train$HOME_VAL %>% replace_na(means[2])
insurance_train$BLUEBOOK <- insurance_train$BLUEBOOK %>% replace_na(means[3])
insurance_train$OLDCLAIM <- insurance_train$OLDCLAIM %>% replace_na(means[4])
insurance_train$AGE <- insurance_train$AGE %>% replace_na(means[5])
insurance_train$YOJ <- insurance_train$YOJ %>% replace_na(means[6])
insurance_train$CAR_AGE <- insurance_train$CAR_AGE %>% replace_na(means[7])
insurance_train$CAR_AGE[insurance_train$CAR_AGE == -3] <- 0
# categorical variables to have z_ in the value replaced
categorical_vars <-
c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")
# replace the z_ with the empty string using apply and gsub
insurance_train[, categorical_vars] <-
apply(insurance_train[, categorical_vars], 2,
function(x) as.factor(gsub("z_", "", x)))
insurance_train[, categorical_vars] <- lapply(insurance_train[, categorical_vars],
factor)
# replace "" a factor label in JOB and replace with other to mean other jobs
levels(insurance_train$JOB)[levels(insurance_train$JOB) == ""] <- "Other"
# get histogram of the numerical/integer data first
ggplot(gather(insurance_train[, c(2:7, 15, 18, 22, 24:25)]), aes(x=value)) +
geom_histogram(bins = 10) +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Numerical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6))
# get plot of the categorical variables
ggplot(gather(insurance_train[, c(9, 11:14, 16, 19:20, 23, 26)]), aes(x=value)) +
geom_bar() +
facet_wrap(~ key, scales = "free") +
theme_bw() +
theme_classic() +
ggtitle("Histogram of All Categorical Attributes") +
theme(axis.title = element_blank(), axis.text =element_text(size = 6),
axis.text.x = element_text(angle = 45, hjust = 1))
# get rid of the index it is no use to us for this analysis
insurance_train <- insurance_train %>% select(-c(INDEX))
# use a model that includes all variables and do a prediction
# the squared inverse of these predictions will be used as weights for the features
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train)
pred <- predict(lm1_insurance, newdata=insurance_train) # make predictions
# re-fit model but using weights
lm1_insurance <- lm(TARGET_AMT ~., data = insurance_train, weights = abs(pred))
summary(lm1_insurance)
par(mfrow=c(2,2))
plot(lm1_insurance)
lm2_insurance <- step(lm1_insurance, direction = "backward", trace = FALSE)
summary(lm2_insurance)
par(mfrow=c(2,2))
plot(lm2_insurance)
# make a logistic regression model then use step() to see which variables
# are most desirable to be used
insurance_train_subset <- insurance_train %>% select(-c(TARGET_AMT))
lm1_log_insurance <- glm(TARGET_FLAG ~.,
data = insurance_train_subset, family = "binomial")
lm1_log_insurance <- stepAIC(lm1_log_insurance, direction = "backward", trace = FALSE)
summary(lm1_log_insurance)
lm2_log_insurance <- cv.glmnet(data.matrix(insurance_train[, c(3:25)]),
y = insurance_train$TARGET_FLAG, family = "binomial",
type.measure = "class")
# coefficents
plot(lm2_log_insurance)
fit <- glmnet(data.matrix(insurance_train[, c(3:25)]),
y = insurance_train$TARGET_FLAG, family = "binomial",
alpha = 1, lambda = lm2_log_insurance$lambda.1se)
# Fitted coefficents
fit$a0
fit$beta[,1][fit$beta[,1] != 0]
# computing the AIC
k <- fit$df # number of features, degrees of freedom
loglikhood <- fit$nulldev - deviance(fit) # log-likihood
AICc <- -2*loglikhood + 2*k
AICc
lm3_model_insurance <- glm(TARGET_FLAG ~ AGE + CAR_USE + CLM_FREQ +
KIDSDRIV + MVR_PTS + OLDCLAIM + REVOKED + SEX + TIF +
TRAVTIME, data = insurance_train, family = "binomial")
summary(lm3_model_insurance)
sm <- summary(lm2_insurance)
mse <- mean(sm$residuals^2)
mse
sm$r.squared
sm$fstatistic[1]
par(mfrow=c(2,2))
plot(lm2_insurance)
# predict on the training data to compute stats
predict_train <- predict(fit, type = "class", newx = data.matrix(insurance_train[,3:25]),
s = "lambda.1se")
table(predict_train)
table(insurance_train$TARGET_FLAG)
conf_matrix <- confusionMatrix(factor(predict_train), insurance_train$TARGET_FLAG,
positive = "1")
?confusionMatrix()
7241+920
6008+2153
lm2_insurance
?cbind
wine <- read.csv("wine-training-data.csv")
setwd("~/Desktop/CUNYSPS/DATA621/Homework5")
wine <- read.csv("wine-training-data.csv")
str(wine)
summary(wine)
apply(wine, 2, mean, na.rm=TRUE)
apply(wine, 2, mean, length, is.na)
apply(wine, 2, is.na)
complete.cases(wine)
library((dplyr))
library(dplyr)
colSums(wine, na.rm = TRUE)
apply(wine, 2, sum, na.rm = TRUE)
is.na(wine)
colsums(is.na(wine))
colSums(is.na(wine))
summary(wine)
colSums(is.na(wine)) > 0
wine[, colSums(is.na(wine)) > 0]
cols_na <- colnames(wine)[colSums(is.na(wine)) > 0]
apply(wine[, cols_na], 2, mean, na.rm = TRUE)
wine[is.na(wine), cols_na]
wine[is.na(wine), cols_na] <- means <- apply(wine[, cols_na], 2, mean, na.rm = TRUE)
means
wine[is.na(wine), cols_na] <- rep(means, nrow(wine[is.na(wine), cols_na]))
rep(means, nrow(wine[is.na(wine), cols_na]))
replace_na
wine %>% replace(., is.na(.), cols_na)
wine <- read.csv("wine-training-data.csv")
cols_na
means
typeof(means)
means[1]
names(means)
library(tidyr)
wine %>% replace_na(means)
wine
means
wine %>% replace_na(means)
wine %>% replace_na(list(means))
wine <- wine %>% replace_na(list(means))
summary(wine)
wine[]
wine <- lapply(wine, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
summary(wine)
wine <- read.csv("wine-training-data.csv")
summary(wine)
wine <- lapply(wine, function(x) replace(is.na(x), mean(x, na.rm = TRUE), x))
summary(wine)
wine <- read.csv("wine-training-data.csv")
cols_na
for(i in 1:ncol(wine[, cols_na])){}
for(i in 1:ncol(wine[, cols_na])){ }
for(i in 1:ncol(wine[, cols_na])){
wine[, i] <- means[i]
}
summary(wine)
wine <- read.csv("wine-training-data.csv")
for(i in 1:ncol(wine[, cols_na])){
}
wine[is.na(wine),cols_na]
x <- wine[is.na(wine),cols_na]
for(i in 1:ncol(x)){
x[,i] <- means[i]
}
wine
x
summary(x)
summary(wine)
for(x in cols_na){
print(x)
}
means$STARS
means
names(means)
means$STARS
for(i in means){i}
for(i in means){print(i)}
means[1]
means[1,]
dim(means)
means
dim(means)
nrow(means)
means[[1]]
means$"STARS"
names(means)
as.integer(2.042)
table(wine$STARS)
table(is.na(wine$STARS))
library(reshape2)
melt(wine, id.vars = 'TARGET')
melt(mtcars, id.vars = 'mpg')
names(mtcars)
mtcars$carb
mtcars$carb,mtcars$mpg
c(mtcars$carb,mtcars$mpg)
cbind(mtcars$carb,mtcars$mpg)
?geom_smooth
melt(mtcars, id.vars = 'mpg')
melt(wine, id.vars = 'TARGET')
pois_lm <- glm(TARGET ~., data = wine, method = "poisson")
mean(wine$TARGET, na.rm = TRUE)
var(wine$TARGET, na.rm = TRUE)
rpois(500,0.5)
rpois(500,0.05)
rpois(500,5)
x<-rpois(500,5)
?rnbinom
unique(wine$TARGET)
library(glmnet)
mean(wine, na.rm = TRUE)
mean(wine$TARGET, na.rm = TRUE)
var(wine$TARGET, na.rm = TRUE)
sqrt(var(wine$TARGET, na.rm = TRUE))
var(sqrt(wine$TARGET), na.rm = TRUE)
mean(sqrt(wine$TARGET), na.rm = TRUE)
R_squared <- 1 - cv.fit$cvm/var(wine$TARGET)
unique(wine$TARGET)
table(wine$TARGET)
